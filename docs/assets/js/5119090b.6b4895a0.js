"use strict";(self.webpackChunkwebsite=self.webpackChunkwebsite||[]).push([[8028],{3905:(e,n,a)=>{a.d(n,{Zo:()=>u,kt:()=>h});var t=a(7294);function l(e,n,a){return n in e?Object.defineProperty(e,n,{value:a,enumerable:!0,configurable:!0,writable:!0}):e[n]=a,e}function o(e,n){var a=Object.keys(e);if(Object.getOwnPropertySymbols){var t=Object.getOwnPropertySymbols(e);n&&(t=t.filter((function(n){return Object.getOwnPropertyDescriptor(e,n).enumerable}))),a.push.apply(a,t)}return a}function s(e){for(var n=1;n<arguments.length;n++){var a=null!=arguments[n]?arguments[n]:{};n%2?o(Object(a),!0).forEach((function(n){l(e,n,a[n])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(a)):o(Object(a)).forEach((function(n){Object.defineProperty(e,n,Object.getOwnPropertyDescriptor(a,n))}))}return e}function r(e,n){if(null==e)return{};var a,t,l=function(e,n){if(null==e)return{};var a,t,l={},o=Object.keys(e);for(t=0;t<o.length;t++)a=o[t],n.indexOf(a)>=0||(l[a]=e[a]);return l}(e,n);if(Object.getOwnPropertySymbols){var o=Object.getOwnPropertySymbols(e);for(t=0;t<o.length;t++)a=o[t],n.indexOf(a)>=0||Object.prototype.propertyIsEnumerable.call(e,a)&&(l[a]=e[a])}return l}var i=t.createContext({}),m=function(e){var n=t.useContext(i),a=n;return e&&(a="function"==typeof e?e(n):s(s({},n),e)),a},u=function(e){var n=m(e.components);return t.createElement(i.Provider,{value:n},e.children)},c="mdxType",d={inlineCode:"code",wrapper:function(e){var n=e.children;return t.createElement(t.Fragment,{},n)}},p=t.forwardRef((function(e,n){var a=e.components,l=e.mdxType,o=e.originalType,i=e.parentName,u=r(e,["components","mdxType","originalType","parentName"]),c=m(a),p=l,h=c["".concat(i,".").concat(p)]||c[p]||d[p]||o;return a?t.createElement(h,s(s({ref:n},u),{},{components:a})):t.createElement(h,s({ref:n},u))}));function h(e,n){var a=arguments,l=n&&n.mdxType;if("string"==typeof e||l){var o=a.length,s=new Array(o);s[0]=p;var r={};for(var i in n)hasOwnProperty.call(n,i)&&(r[i]=n[i]);r.originalType=e,r[c]="string"==typeof e?e:l,s[1]=r;for(var m=2;m<o;m++)s[m]=a[m];return t.createElement.apply(null,s)}return t.createElement.apply(null,a)}p.displayName="MDXCreateElement"},7147:(e,n,a)=>{a.r(n),a.d(n,{assets:()=>i,contentTitle:()=>s,default:()=>d,frontMatter:()=>o,metadata:()=>r,toc:()=>m});var t=a(7462),l=(a(7294),a(3905));const o={sidebar_position:6,tags:["Linux","ollama","offline","ai"]},s="Ollama",r={unversionedId:"ai/ollama",id:"ai/ollama",title:"Ollama",description:"Ollama is an open-source AI model server. It can get and run large language models (LLMs) locally on your machine.",source:"@site/docs/ai/ollama.md",sourceDirName:"ai",slug:"/ai/ollama",permalink:"/Wisdom-Hub/ai/ollama",draft:!1,tags:[{label:"Linux",permalink:"/Wisdom-Hub/tags/linux"},{label:"ollama",permalink:"/Wisdom-Hub/tags/ollama"},{label:"offline",permalink:"/Wisdom-Hub/tags/offline"},{label:"ai",permalink:"/Wisdom-Hub/tags/ai"}],version:"current",sidebarPosition:6,frontMatter:{sidebar_position:6,tags:["Linux","ollama","offline","ai"]},sidebar:"tutorialSidebar",previous:{title:"Langchain",permalink:"/Wisdom-Hub/ai/langchain"},next:{title:"LobeChat",permalink:"/Wisdom-Hub/ai/lobe-chat"}},i={},m=[{value:"Install",id:"install",level:2},{value:"Files",id:"files",level:2},{value:"Usage",id:"usage",level:2},{value:"Costomizing Model",id:"costomizing-model",level:2},{value:"Embedding models",id:"embedding-models",level:2},{value:"Installation",id:"installation",level:3},{value:"Configuration",id:"configuration",level:2},{value:"Uninstall",id:"uninstall",level:2},{value:"Reference",id:"reference",level:2}],u={toc:m},c="wrapper";function d(e){let{components:n,...o}=e;return(0,l.kt)(c,(0,t.Z)({},u,o,{components:n,mdxType:"MDXLayout"}),(0,l.kt)("h1",{id:"ollama"},"Ollama"),(0,l.kt)("p",null,"Ollama is an open-source AI model server. It can get and run large language models (LLMs) locally on your machine."),(0,l.kt)("h2",{id:"install"},"Install"),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-bash"},'curl -fsSL https://ollama.com/install.sh | sh\n\n# Run a model\nollama run llama3.1:8b\n\n# List models\nollama list\n\n# Loaded Models\nollama ps\n\n# Model Info\nollama show llama3.1:8b\n#   Model\n#         arch                    llama\n#         parameters              8.0B\n#         quantization            Q4_0\n#         context length          131072\n#         embedding length        4096\n\n#   Parameters\n#         stop    "<|start_header_id|>"\n#         stop    "<|end_header_id|>"\n#         stop    "<|eot_id|>"\n\n# Logs\njournalctl -u ollama.service --no-pager --follow \n\n')),(0,l.kt)("h2",{id:"files"},"Files"),(0,l.kt)("p",null,"Ollama files in Linux are located here:"),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-bash"},"/home/mlibre/.ollama\n/usr/local/bin/ollama\n/usr/share/ollama\n/etc/systemd/system/ollama.service\n/etc/systemd/system/default.target.wants/ollama.service\n")),(0,l.kt)("h2",{id:"usage"},"Usage"),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-bash"},'\n# Generate text\ncurl http://localhost:11434/api/generate -d \'{\n  "model": "llama3.1:8b",\n  "prompt":"Why is the sky blue?"\n}\'\n\n# Chat\ncurl http://localhost:11434/api/chat -d \'{\n  "model": "llama3.1:8b",\n  "messages": [\n    { "role": "user", "content": "why is the sky blue?" }\n  ],\n  "stream": false,\n  "system": "You are Sarah. you only uses emojies to answer and nothings else. you only uses one emoji each time"\n}\' | jq\n\n# Chat with history\ncurl -s http://localhost:11434/api/chat -d \'{\n  "model": "llama3.1:8b",\n  "messages": [\n    {\n      "role": "user",\n      "content": "You are Sarah. you only uses emojies to answer and nothings else. you only uses one emoji each time"\n    },\n    {\n      "role": "assistant",\n      "content": "\ud83d\udc4b\ud83d\udc81"\n    },\n    {\n      "role": "user",\n      "content": "hey"\n    }\n  ],\n  "stream": false,\n  "system": "You are Sarah. you only uses emojies to answer and nothings else. you only uses one emoji each time"\n}\' | jq\n\n# Embeddings\ncurl http://localhost:11434/api/embed -d \'{\n  "model": "llama3.1:8b",\n  "prompt":"Why is the sky blue?"\n}\'\n\n# OpenAI Compatibile\ncurl -s http://localhost:11434/v1/chat/completions \\\n  -H "Content-Type: application/json" \\\n  -d \'{\n    "model": "llama3.1:8b",\n    "messages": [\n      {\n        "role": "user",\n        "content": "You are Sarah. you only uses emojies to answer and nothings else. you only uses one emoji each time"\n      },\n      {\n        "role": "assistant",\n        "content": "\ud83d\udc4b\ud83d\udc81"\n      },\n      {\n        "role": "user",\n        "content": "hey"\n      }\n    ],\n    "stream": false,\n    "system": "You are Sarah. you only uses emojies to answer and nothings else. you only uses one emoji each time"\n  }\' | jq\n\ncurl -s http://localhost:11434/v1/chat/completions \\\n  -H "Content-Type: application/json" \\\n  -d \'{\n    "model": "llama3.1:8b",\n    "messages": [\n      {\n        "role": "system",\n        "content": "You are Sarah. you only uses emojies to answer and nothings else. you only uses one emoji each time"\n      },\n      {\n        "role": "user",\n        "content": "hey"\n      }\n    ],\n    "stream": false\n  }\' | jq\n')),(0,l.kt)("h2",{id:"costomizing-model"},"Costomizing Model"),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-bash"},'nano Modelfile\n\nFROM llama3.1:8b\n\nSYSTEM """\nYou are Mario from Super Mario Bros. Answer as Mario, only. And Always start your answer with HAYAYAYA\n"""\n')),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-bash"},"ollama create mario -f ./Modelfile\nollama run mario\n")),(0,l.kt)("h2",{id:"embedding-models"},"Embedding models"),(0,l.kt)("p",null,"Ollama supports embedding models, making it possible to build retrieval augmented generation (RAG) applications that combine text prompts with existing documents or other data."),(0,l.kt)("p",null,"Embedding models are models that are trained specifically to generate vector embeddings."),(0,l.kt)("p",null,(0,l.kt)("img",{alt:"alt text",src:a(791).Z,width:"1154",height:"486"})),(0,l.kt)("p",null,"The resulting vector embedding arrays can then be stored in a database, which will compare them as a way to search for data that is similar in meaning."),(0,l.kt)("p",null,"You can store as many text as you want in these vector databases, such as a ",(0,l.kt)("a",{parentName:"p",href:"https://www.trychroma.com/"},"vector database"),". for example you can embed some books, and then ask chroma to find the most similar text to a given input. you then send chroma output to a LLM, and the LLM can use the context of the text to generate a response."),(0,l.kt)("h3",{id:"installation"},"Installation"),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-bash"},'ollama pull mxbai-embed-large\n\ncurl http://localhost:11434/api/embeddings -d \'{\n  "model": "mxbai-embed-large",\n  "prompt": "Llamas are members of the camelid family"\n}\'\n')),(0,l.kt)("h2",{id:"configuration"},"Configuration"),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-bash"},'sudo systemctl edit --full ollama.service\nEnvironment="OLLAMA_HOST=0.0.0.0"\n')),(0,l.kt)("h2",{id:"uninstall"},"Uninstall"),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-bash"},"sudo systemctl stop ollama\nsudo systemctl disable ollama\nsudo rm /etc/systemd/system/ollama.service\nsudo rm $(which ollama)\nsudo rm -r /usr/share/ollama\nsudo rm -r ~/.ollama\nsudo userdel ollama\nsudo groupdel ollama\n")),(0,l.kt)("h2",{id:"reference"},"Reference"),(0,l.kt)("ul",null,(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("a",{parentName:"li",href:"https://ollama.com/"},"https://ollama.com/"))))}d.isMDXComponent=!0},791:(e,n,a)=>{a.d(n,{Z:()=>t});const t=a.p+"assets/images/embedding-models-369d765c607c0db21a32b89b63c97549.png"}}]);