"use strict";(self.webpackChunkwebsite=self.webpackChunkwebsite||[]).push([[3020],{3905:(e,t,n)=>{n.d(t,{Zo:()=>u,kt:()=>h});var a=n(7294);function i(e,t,n){return t in e?Object.defineProperty(e,t,{value:n,enumerable:!0,configurable:!0,writable:!0}):e[t]=n,e}function r(e,t){var n=Object.keys(e);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(e);t&&(a=a.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),n.push.apply(n,a)}return n}function o(e){for(var t=1;t<arguments.length;t++){var n=null!=arguments[t]?arguments[t]:{};t%2?r(Object(n),!0).forEach((function(t){i(e,t,n[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(n)):r(Object(n)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(n,t))}))}return e}function s(e,t){if(null==e)return{};var n,a,i=function(e,t){if(null==e)return{};var n,a,i={},r=Object.keys(e);for(a=0;a<r.length;a++)n=r[a],t.indexOf(n)>=0||(i[n]=e[n]);return i}(e,t);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);for(a=0;a<r.length;a++)n=r[a],t.indexOf(n)>=0||Object.prototype.propertyIsEnumerable.call(e,n)&&(i[n]=e[n])}return i}var l=a.createContext({}),c=function(e){var t=a.useContext(l),n=t;return e&&(n="function"==typeof e?e(t):o(o({},t),e)),n},u=function(e){var t=c(e.components);return a.createElement(l.Provider,{value:t},e.children)},p="mdxType",d={inlineCode:"code",wrapper:function(e){var t=e.children;return a.createElement(a.Fragment,{},t)}},m=a.forwardRef((function(e,t){var n=e.components,i=e.mdxType,r=e.originalType,l=e.parentName,u=s(e,["components","mdxType","originalType","parentName"]),p=c(n),m=i,h=p["".concat(l,".").concat(m)]||p[m]||d[m]||r;return n?a.createElement(h,o(o({ref:t},u),{},{components:n})):a.createElement(h,o({ref:t},u))}));function h(e,t){var n=arguments,i=t&&t.mdxType;if("string"==typeof e||i){var r=n.length,o=new Array(r);o[0]=m;var s={};for(var l in t)hasOwnProperty.call(t,l)&&(s[l]=t[l]);s.originalType=e,s[p]="string"==typeof e?e:i,o[1]=s;for(var c=2;c<r;c++)o[c]=n[c];return a.createElement.apply(null,o)}return a.createElement.apply(null,n)}m.displayName="MDXCreateElement"},4226:(e,t,n)=>{n.r(t),n.d(t,{assets:()=>l,contentTitle:()=>o,default:()=>d,frontMatter:()=>r,metadata:()=>s,toc:()=>c});var a=n(7462),i=(n(7294),n(3905));const r={sidebar_position:2,tags:["Network","AI","Neural"]},o="Neural Network",s={unversionedId:"ai/neural-network",id:"ai/neural-network",title:"Neural Network",description:"Install Packages",source:"@site/docs/ai/neural-network.md",sourceDirName:"ai",slug:"/ai/neural-network",permalink:"/Wisdom-Hub/ai/neural-network",draft:!1,tags:[{label:"Network",permalink:"/Wisdom-Hub/tags/network"},{label:"AI",permalink:"/Wisdom-Hub/tags/ai"},{label:"Neural",permalink:"/Wisdom-Hub/tags/neural"}],version:"current",sidebarPosition:2,frontMatter:{sidebar_position:2,tags:["Network","AI","Neural"]},sidebar:"tutorialSidebar",previous:{title:"Python",permalink:"/Wisdom-Hub/ai/python-for-ai"},next:{title:"Geenrative AI",permalink:"/Wisdom-Hub/ai/generative-ai"}},l={},c=[{value:"Install Packages",id:"install-packages",level:2},{value:"Check GPU Support",id:"check-gpu-support",level:2},{value:"Concepts",id:"concepts",level:2},{value:"Neuron",id:"neuron",level:3},{value:"Activation Function",id:"activation-function",level:3},{value:"Linear Activation Function",id:"linear-activation-function",level:4},{value:"Sigmoid Activation Function",id:"sigmoid-activation-function",level:4},{value:"Loss Function",id:"loss-function",level:3},{value:"Gradient",id:"gradient",level:3},{value:"Gradient Descent",id:"gradient-descent",level:3},{value:"Batch Gradient Descent",id:"batch-gradient-descent",level:4},{value:"Stochastic Gradient Descent (SGD)",id:"stochastic-gradient-descent-sgd",level:4},{value:"Mini-Batch Gradient Descent",id:"mini-batch-gradient-descent",level:4},{value:"Resources",id:"resources",level:2}],u={toc:c},p="wrapper";function d(e){let{components:t,...n}=e;return(0,i.kt)(p,(0,a.Z)({},u,n,{components:t,mdxType:"MDXLayout"}),(0,i.kt)("h1",{id:"neural-network"},"Neural Network"),(0,i.kt)("h2",{id:"install-packages"},"Install Packages"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-bash"},"pip install --upgrade pip --break-system-packages\n# If you have permission issues\n# sudo chmod a+rwx /usr/lib/python3.12/ -R\n\n# https://wiki.archlinux.org/title/GPGPU\nsudo pamac install opencl-amd --no-confirm\n# Or\n# sudo pamac install rocm-core rocm-hip-sdk rocm-opencl-sdk --no-confirm\nsudo usermod -a -G render,video $LOGNAME\nrocminfo\n\n# If you are using RDNA or RDNA 2 architecture like AMD Radeon RX 6500 XT, you may need to follow this step:\nsudo nano ~/.profile\n# Add the following lines:\nexport HSA_OVERRIDE_GFX_VERSION=10.3.0\nexport ROC_ENABLE_PRE_VEGA=1\n\n\n# https://www.tensorflow.org/install/pip\npip uninstall tensorflow-rocm numpy\npip install tensorflow --break-system-packages\n\npip install https://repo.radeon.com/rocm/manylinux/rocm-rel-6.1.3/tensorflow_rocm-2.15.1-cp310-cp310-manylinux_2_28_x86_64.whl numpy==1.26.4 --break-system-packages\n\n\n# https://pytorch.org/\npip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/rocm6.1 --break-system-packages\n")),(0,i.kt)("h2",{id:"check-gpu-support"},"Check GPU Support"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},"import tensorflow as tf   # TensorFlow registers PluggableDevices here\nimport torch\n\nprint(tf.config.list_physical_devices())\nprint(tf.__version__)\n\nprint(torch.cuda.is_available())\nprint(torch.version.hip)\n")),(0,i.kt)("h2",{id:"concepts"},"Concepts"),(0,i.kt)("h3",{id:"neuron"},"Neuron"),(0,i.kt)("p",null,"A neuron is a function that takes some inputs, applies a weighted sum to them, and then generates an output using an activation function"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},'def neuronFunc(inputs):\n    weights = [0.5, -0.5]\n    bias = 0.0\n    output = inputs[0] * weights[0] + inputs[1] * weights[1] + bias\n    return output\n\nneuronOut = neuronFunc([1.0, 2.0])\nprint("Neuron output:", neuronOut)\n')),(0,i.kt)("h3",{id:"activation-function"},"Activation Function"),(0,i.kt)("p",null,"An activation function is applied to the output of a neuron to introduce non-linearity into the model. This helps the neural network learn complex patterns"),(0,i.kt)("h4",{id:"linear-activation-function"},"Linear Activation Function"),(0,i.kt)("p",null,"The linear activation function directly outputs the input value without any modification."),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},'def linear(x):\n    return x\n\nactiveOutput = linear(neuronOut)\nprint("Activated output (linear):", activeOutput)\n')),(0,i.kt)("h4",{id:"sigmoid-activation-function"},"Sigmoid Activation Function"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},'import math\n\ndef sigmoid(x):\n    return 1 / (1 + math.exp(-x)) # e^-x\n\n# Apply the activation function\nactiveOutput = sigmoid(neuronOut)\nprint("Activated output (sigmoid):", activeOutput)\n')),(0,i.kt)("h3",{id:"loss-function"},"Loss Function"),(0,i.kt)("p",null,"A loss function measures how well a neural network model performs a certain task by calculating the difference between the predicted output and the actual output. The goal of training is to minimize this loss."),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},'y_true = 0.8\nloss = (y_true - activeOutput)\nprint("Loss:", loss)\n')),(0,i.kt)("h3",{id:"gradient"},"Gradient"),(0,i.kt)("p",null,"In neural networks, gradients are used to update the model parameters to minimize the loss function"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},'def f(x):\n    return x**2\n\ndef gradient(f, x, delta_x=0.00001):\n    return (f(x + delta_x) - f(x)) / delta_x\n\nx = 3\ngrad = gradient(f, x)\nprint("Gradient of f at x = {}: {}".format(x, grad))\n\n')),(0,i.kt)("h3",{id:"gradient-descent"},"Gradient Descent"),(0,i.kt)("p",null,"Gradient Descent is an optimization algorithm used to minimize the loss function by iteratively moving towards the steepest descent direction defined by the negative of the gradient."),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},'def gradient_descent(f, starting_point, learning_rate, num_iterations):\n    x = starting_point\n    for _ in range(num_iterations):\n        grad = gradient(f, x)\n        x = x - (learning_rate * grad)\n    return x\n\nstarting_point = 3\nlearning_rate = 0.1\nnum_iterations = 100\noptimal_x = gradient_descent(f, starting_point, learning_rate, num_iterations)\nprint("Optimal x:", optimal_x)\n')),(0,i.kt)("p",null,"There are 3 types of gradient descent:"),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},"Batch Gradient Descent"),(0,i.kt)("li",{parentName:"ul"},"Stochastic Gradient Descent"),(0,i.kt)("li",{parentName:"ul"},"Mini-Batch Gradient Descent")),(0,i.kt)("h4",{id:"batch-gradient-descent"},"Batch Gradient Descent"),(0,i.kt)("p",null,"Batch Gradient Descent computes the gradient of the loss function with respect to the entire training dataset. It updates the parameters in the direction of the gradient computed from the entire dataset"),(0,i.kt)("h4",{id:"stochastic-gradient-descent-sgd"},"Stochastic Gradient Descent (SGD)"),(0,i.kt)("p",null,"Stochastic Gradient Descent updates the parameters for each training example, rather than the entire dataset, which often leads to faster convergence."),(0,i.kt)("h4",{id:"mini-batch-gradient-descent"},"Mini-Batch Gradient Descent"),(0,i.kt)("p",null,"Mini-Batch Gradient Descent strikes a balance between Batch Gradient Descent and Stochastic Gradient Descent by updating the parameters using small batches of the training dataset."),(0,i.kt)("h2",{id:"resources"},"Resources"))}d.isMDXComponent=!0}}]);